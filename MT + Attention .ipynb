{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translate your own language\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch, torchdata, torchtext\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random, math, time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.14.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      source                                   english_sentence  \\\n",
      "0        ted  politicians do not have permission to do what ...   \n",
      "1        ted         I'd like to tell you about one such child,   \n",
      "2  indic2012  This percentage is even greater than the perce...   \n",
      "3        ted  what we really mean is that they're bad at not...   \n",
      "4  indic2012  .The ending portion of these Vedas is called U...   \n",
      "\n",
      "                                      hindi_sentence  \n",
      "0  राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह कर...  \n",
      "1  मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहू...  \n",
      "2   यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।  \n",
      "3     हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते  \n",
      "4        इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।  \n",
      "(127607, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/ENg_Dzo_Translation/Hindi_English_Truncated_Corpus.csv')\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_en = df['english_sentence']\n",
    "data_hi = df['hindi_sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(127607, 127607)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_en), len(data_hi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(data[0], data[1]) for data in zip(data_en, data_hi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_size = len(data)\n",
    "total_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politicians do not have permission to do what needs to be done. राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .\n"
     ]
    }
   ],
   "source": [
    "for en, hi in data:\n",
    "  print(en, hi)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('politicians do not have permission to do what needs to be done.', 'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .')\n",
      "(\"I'd like to tell you about one such child,\", 'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,')\n",
      "('This percentage is even greater than the percentage in India.', 'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।')\n",
      "(\"what we really mean is that they're bad at not paying attention.\", 'हम ये नहीं कहना चाहते कि वो ध्यान नहीं दे पाते')\n",
      "('.The ending portion of these Vedas is called Upanishad.', 'इन्हीं वेदों का अंतिम भाग उपनिषद कहलाता है।')\n",
      "('The then Governor of Kashmir resisted transfer , but was finally reduced to subjection with the aid of British .', 'कश्मीर के तत्कालीन गवर्नर ने इस हस्तांतरण का विरोध किया था , लेकिन अंग्रेजों की सहायता से उनकी आवाज दबा दी गयी .')\n"
     ]
    }
   ],
   "source": [
    "for count, i in enumerate(data):\n",
    "    print(i)\n",
    "    if count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)\n",
    "train_size = int(total_size *0.8)\n",
    "val_size = int(train_size*0.2)\n",
    "\n",
    "test = data[train_size:]\n",
    "train_dataset = data[:train_size]\n",
    "val = train_dataset[:val_size]\n",
    "train = train_dataset[val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 1600 2000\n"
     ]
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "val_size = len(list(iter(val)))\n",
    "test_size = len(list(iter(test)))\n",
    "print(train_size, val_size, test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'en'\n",
    "TRG_LANGUAGE = 'hi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='de_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  इसलिए उन्होंने इस अफवाह को फैलानेवालों के विरुद्ध कुछ नहीं कहा जो यह बता रहे थे कि इस भारतीय कवि का जर्मन प्रोपेगंडा द्वारा इस्तेमाल किया जा रहा है तथा अमेरिका की सहानुभूति को ब्रिटिश और मित्र देशों के युद्ध प्रयासों के विरुद्ध अलग-थलग करना चाह रहा है .\n",
      "Tokenization:  ['इसलिए', 'उन्होंने', 'इस', 'अफवाह', 'को', 'फैलानेवालों', 'के', 'विरुद्ध', 'कुछ', 'नहीं', 'कहा', 'जो', 'यह', 'बता', 'रहे', 'थे', 'कि', 'इस', 'भारतीय', 'कवि', 'का', 'जर्मन', 'प्रोपेगंडा', 'द्वारा', 'इस्तेमाल', 'किया', 'जा', 'रहा', 'है', 'तथा', 'अमेरिका', 'की', 'सहानुभूति', 'को', 'ब्रिटिश', 'और', 'मित्र', 'देशों', 'के', 'युद्ध', 'प्रयासों', 'के', 'विरुद्ध', 'अलग-थलग', 'करना', 'चाह', 'रहा', 'है', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", train[0][1])\n",
    "print(\"Tokenization: \", token_transform[TRG_LANGUAGE](train[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55, 12, 25, 162, 784]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['who', 'is', 'this', 'man', 'whose'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 998, for example\n",
    "mapping[998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6689"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[185, 1521, 483, 1317, 14]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vocab_transform[TRG_LANGUAGE](['मुझे', 'सुंदर', 'रंग', 'पसंद', 'हैं'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "चीनी\n",
      "7034\n"
     ]
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping_ne = vocab_transform[TRG_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1001, for example\n",
    "print(mapping_ne[1001])\n",
    "print(len(mapping_ne))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val, batch_size=batch_size,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test, batch_size=batch_size,\n",
    "                             shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hi, _, en in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi shape:  torch.Size([64, 111])\n",
      "English shape:  torch.Size([64, 117])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Hindi shape: \", hi.shape)  # (seq len, batch_size)\n",
    "print(\"English shape: \", en.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(train_loader)\n",
    "val_loader_length   = len(valid_loader)\n",
    "test_loader_length  = len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 25, 32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader_length, val_loader_length, test_loader_length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "### Encoder\n",
    "\n",
    "The changes here all within the forward method. It now accepts the lengths of the source sentences as well as the sentences themselves.\n",
    "\n",
    "After the source sentence (padded automatically within the iterator) has been embedded, we can then use pack_padded_sequence on it with the lengths of the sentences. Note that the tensor containing the lengths of the sequences must be a CPU tensor as of the latest version of PyTorch, which we explicitly do so with to('cpu'). packed_embedded will then be our packed padded sequence. This can be then fed to our RNN as normal which will return packed_outputs, a packed tensor containing all of the hidden states from the sequence, and hidden which is simply the final hidden state from our sequence. hidden is a standard tensor and not packed in any way, the only difference is that as the input was a packed sequence, this tensor is from the final non-padded element in the sequence.\n",
    "\n",
    "We then unpack our packed_outputs using pad_packed_sequence which returns the outputs and the lengths of each, which we don't need.\n",
    "\n",
    "The first dimension of outputs is the padded sequence lengths however due to using a packed padded sequence the values of tensors when a padding token was the input will be all zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn       = nn.GRU(emb_dim, hid_dim, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hid_dim * 2, hid_dim)\n",
    "        self.dropout   = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_len):\n",
    "        \n",
    "        #src: [src len, batch size]\n",
    "        #src len: [batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, \n",
    "                            src_len.to('cpu'), enforce_sorted=False)\n",
    "        \n",
    "        #packed_outputs contain all hidden states including padding guy\n",
    "        #hidden contains the last hidden states of the non-padded guys\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        #hidden: [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #convert packed_outputs to the guy that does not contain hidden states for padding\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #outputs: [src len, batch size, hid dim * num directions]\n",
    "        \n",
    "        #take the last hidden states from backward and forward\n",
    "        #hidden: (f, b, f, b)\n",
    "        forward  = hidden[-2, :, :]  #[batch size, hid dim]\n",
    "        backward = hidden[-1, :, :]  #[batch size, hid dim]\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(torch.cat((forward, backward), dim = 1))) \n",
    "        #hidden: [batch size, hid dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention used here is additive attention which is defined by:\n",
    "\n",
    "Previously, we allowed this module to \"pay attention\" to padding tokens within the source sentence. However, using masking, we can force the attention to only be over non-padding elements.\n",
    "\n",
    "The forward method now takes a mask input. This is a [batch size, source sentence length] tensor that is 1 when the source sentence token is not a padding token, and 0 when it is a padding token. For example, if the source sentence is: [\"hello\", \"how\", \"are\", \"you\", \"?\",,], then the mask would be [1, 1, 1, 1, 1, 0, 0].\n",
    "\n",
    "We apply the mask after the attention has been calculated, but before it has been normalized by the softmax function. It is applied using masked_fill. This fills the tensor at each element where the first argument (mask == 0) is true, with the value given by the second argument (-1e10). In other words, it will take the un-normalized attention values, and change the attention values over padded elements to be -1e10. As these numbers will be miniscule compared to the other values they will become zero when passed through the softmax layer, ensuring no attention is payed to padding tokens in the source sentence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim, variants):\n",
    "        super().__init__()\n",
    "        self.variants = variants\n",
    "        self.v = nn.Linear(hid_dim, 1, bias = False)\n",
    "        self.W = nn.Linear(hid_dim,     hid_dim) #for decoder\n",
    "        self.U = nn.Linear(hid_dim * 2, hid_dim) #for encoder outputs\n",
    "                \n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "\n",
    "        if self.variants == 'additive': #work\n",
    "            #repeat decoder hidden state src_len times\n",
    "            hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "            #hidden = [batch size, src len, hid dim]\n",
    "            \n",
    "            energy = torch.tanh(self.W(hidden) + self.U(encoder_outputs))\n",
    "            #energy = [batch size, src len, hid dim]\n",
    "            \n",
    "            attention = self.v(energy).squeeze(2)\n",
    "            #attention = [batch size, src len]\n",
    "            \n",
    "        elif self.variants == 'general': #work\n",
    "            hidden = hidden.unsqueeze(1).repeat(1, 1, 2)\n",
    "            #hidden = [batch size, 1, hid dim*2]\n",
    "            #encoder_outputs = [batch size, hid dim * 2, src len]\n",
    "\n",
    "            energy = torch.bmm(hidden, encoder_outputs.transpose(1, 2))\n",
    "            attention = energy.squeeze(1)\n",
    "            #attention = [batch size, src len]\n",
    "\n",
    "        elif self.variants == 'multiplicative':\n",
    "            wh = self.W(hidden).unsqueeze(1).repeat(1, 1, 2)\n",
    "            #wh = [batch size, 1, hid dim*2]\n",
    "            #encoder_outputs = [batch size, hid dim * 2, src len]\n",
    "\n",
    "            energy = torch.bmm(wh, encoder_outputs.transpose(1, 2))\n",
    "            attention = energy.squeeze(1)\n",
    "\n",
    "        #use masked_fill_ if you want in-place\n",
    "        attention = attention.masked_fill(mask, -1e10)\n",
    "        #attention = [batch size, src len]\n",
    "        return F.softmax(attention, dim = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[           9, -10000000000,            7,            2, -10000000000,\n",
      "         -10000000000],\n",
      "        [          99, -10000000000, -10000000000,            0,            8,\n",
      "                    9]])\n"
     ]
    }
   ],
   "source": [
    "#example of masked_fill\n",
    "#reall that 1 is pad_idx\n",
    "x = torch.tensor([ [9, 1, 7, 2, 1, 1], [99, 1, 1, 0, 8, 9] ])\n",
    "\n",
    "mask = (x == PAD_IDX)\n",
    "\n",
    "x.masked_fill_(mask, -1e10)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.gru = nn.GRU((hid_dim * 2) + emb_dim, hid_dim)\n",
    "        self.fc = nn.Linear((hid_dim * 2) + hid_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs, mask):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, hid dim]\n",
    "        #encoder_outputs = [src len, batch size, hid dim * 2]\n",
    "        #mask = [batch size, src len]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs, mask)\n",
    "        #a = [batch size, src len]\n",
    "\n",
    "        a = a.unsqueeze(1)\n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs = [batch size, src len, hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs) #Ct\n",
    "        #weighted = [batch size, 1, hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        #weighted = [1, batch size, hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input = [1, batch size, (hid dim * 2) + emb dim]\n",
    "        \n",
    "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        assert (output == hidden).all()\n",
    "\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output   = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0), a.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqPackedAttention(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def create_mask(self, src):\n",
    "        #src: [src len, batch size]\n",
    "        \n",
    "        mask = (src == self.src_pad_idx).permute(1, 0)\n",
    "        #mask: [batch size, src len]\n",
    "        #we need to permute to make the mask same shape as attention...\n",
    "        return mask\n",
    "\n",
    "        \n",
    "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src: [src len, batch size]\n",
    "        #src len: [batch size]\n",
    "        #trg: [trg len, batch size]\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len    = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim #define in decoder\n",
    "        \n",
    "        #because decoder decodes each step....let's create a list that gonna append the result to this guy\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #because decoder decodes each step....let's memorize the attention done in each step....\n",
    "        attentions = torch.zeros(trg_len, batch_size, src.shape[0]).to(self.device)\n",
    "        \n",
    "        #let's start!!!\n",
    "        #1. encoder\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        #encoder_outputs: [src len, batch size, hid dim * num directions]\n",
    "        #hidden: [batch size, hid dim]\n",
    "        \n",
    "        #set the first input to the decoder\n",
    "        input_ = trg[0,:]  #basically \n",
    "        \n",
    "        #create the mask for use in this step\n",
    "        mask = self.create_mask(src)\n",
    "        \n",
    "        #2. for each of trg word\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #3. decode (hidden is always carry forward)\n",
    "            output, hidden, attention = self.decoder(input_, hidden, encoder_outputs, mask)\n",
    "            #output:   [batch size, output_dim]\n",
    "            #hidden:   [batch size, hid_dim]\n",
    "            #attention::[batch size, src len]  ==> how each of src token is important to input_ \n",
    "            \n",
    "            #4. append the results to outputs and attentions\n",
    "            outputs[t] = output\n",
    "            attentions[t] = attention\n",
    "            \n",
    "            #5. get the result, using argmax\n",
    "            top1 = output.argmax(1)  #find the maximum index of dimension 1, which is output_dim\n",
    "            \n",
    "            #6. based on the teacher forcing ratio, \n",
    "            teacher_force_or_not = random.random() < teacher_forcing_ratio\n",
    "                    #if teacher forcing, next input is the next trg\n",
    "                    #if no teacher forcing, the next input is the argmax guy...\n",
    "            input_ = trg[t] if teacher_force_or_not else top1  #autoregressive\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training\n",
    "\n",
    "We can now define our encoder and decoders. This model is significantly smaller than Transformers used in research today, but is able to be run on a single GPU quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper does not mention which weight initialization scheme was used, however Xavier uniform seems to be common amongst Transformer models, so we use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqPackedAttention(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(6689, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (W): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (U): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(7034, 256)\n",
       "    (gru): GRU(1280, 512)\n",
       "    (fc): Linear(in_features=1792, out_features=7034, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "emb_dim     = 256  \n",
    "hid_dim     = 512  \n",
    "dropout     = 0.5\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "\n",
    "attn = Attention(hid_dim, variants='general')\n",
    "enc  = Encoder(input_dim,  emb_dim,  hid_dim, dropout)\n",
    "dec  = Decoder(output_dim, emb_dim,  hid_dim, dropout, attn)\n",
    "\n",
    "model_general = Seq2SeqPackedAttention(enc, dec, SRC_PAD_IDX, device).to(device)\n",
    "model_general.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1712384\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "393216\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "524288\n",
      "   512\n",
      "   512\n",
      "262144\n",
      "   512\n",
      "524288\n",
      "   512\n",
      "1800704\n",
      "1966080\n",
      "786432\n",
      "  1536\n",
      "  1536\n",
      "12604928\n",
      "  7034\n",
      "______\n",
      "22558842\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model_general)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model_general.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_length, trg in loader:\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output, attentions = model(src, src_length, trg)\n",
    "        \n",
    "        #trg    = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        #the loss function only works on 2d inputs with 1d targets thus we need to flatten each of them\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg    = trg[1:].view(-1)\n",
    "        #trg    = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #clip the gradients to prevent them from exploding (a common issue in RNNs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "        \n",
    "    #turn off dropout (and batch norm if used)\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_length, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "\n",
    "            output, attentions = model(src, src_length, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg    = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg    = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "start (1172) + length (1) exceeds dimension size (1172).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 66\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     train_loss \u001b[39m=\u001b[39m train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     valid_loss \u001b[39m=\u001b[39m evaluate(model_general, valid_loader, criterion, val_loader_length)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m#for plotting\u001b[39;00m\n",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 66\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m trg \u001b[39m=\u001b[39m trg\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m output, attentions \u001b[39m=\u001b[39m model(src, src_length, trg)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#trg    = [trg len, batch size]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#output = [trg len, batch size, output dim]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output_dim \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 66\u001b[0m in \u001b[0;36mSeq2SeqPackedAttention.forward\u001b[0;34m(self, src, src_len, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m attentions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(trg_len, batch_size, src\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m#let's start!!!\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m#1. encoder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m encoder_outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(src, src_len)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m#encoder_outputs: [src len, batch size, hid dim * num directions]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m#hidden: [batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39m#set the first input to the decoder\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m input_ \u001b[39m=\u001b[39m trg[\u001b[39m0\u001b[39m,:]  \u001b[39m#basically \u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 66\u001b[0m in \u001b[0;36mEncoder.forward\u001b[0;34m(self, src, src_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m packed_embedded \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpack_padded_sequence(embedded, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     src_len\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m), enforce_sorted\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#packed_outputs contain all hidden states including padding guy\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#hidden contains the last hidden states of the non-padded guys\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m packed_outputs, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(packed_embedded)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#hidden: [n layers * num directions, batch size, hid dim]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m#convert packed_outputs to the guy that does not contain hidden states for padding\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y125sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m outputs, _ \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_packed_sequence(packed_outputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/nn/modules/rnn.py:958\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    955\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    956\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    957\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 958\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, batch_sizes, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    959\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional)\n\u001b[1;32m    960\u001b[0m output \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m]\n\u001b[1;32m    961\u001b[0m hidden \u001b[39m=\u001b[39m result[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: start (1172) + length (1) exceeds dimension size (1172)."
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 10\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'{model_general.__class__.__name__}_general.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model_general, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model_general, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_general.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Seq2SeqPackedAttention_general.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 78\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_general\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(save_path))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test_loss \u001b[39m=\u001b[39m evaluate(model_general, test_loader, criterion, test_loader_length)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y130sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m| Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m | Test PPL: \u001b[39m\u001b[39m{\u001b[39;00mmath\u001b[39m.\u001b[39mexp(test_loss)\u001b[39m:\u001b[39;00m\u001b[39m7.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m |\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Seq2SeqPackedAttention_general.pt'"
     ]
    }
   ],
   "source": [
    "model_general.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model_general, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   2, 2201,    5,    0,   12,   42, 2282,   10,    0, 1261,    6,   51,\n",
       "           34, 2401,   19,  176,   28, 3918,  177,   14,    4, 4131,    9,   11,\n",
       "            0,  101,   24,    0,   36,    4,  982,    7,  103,   55,    0,    5,\n",
       "            3]),\n",
       " tensor([   2, 3795,    7,    0, 6739,  362,    4,  719, 1150,    0,   66, 2648,\n",
       "          489,   18,  722,   20,   21,   93,   35,  122,   18,    0,   11, 6298,\n",
       "          128,   13, 1100,   20,    5,   18,   55, 3793, 1891,   76,    4,  241,\n",
       "         3061,    4, 1233, 1915, 2408,    7,    3]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](data[3][0]).to(device)\n",
    "trg_text = text_transform[TRG_LANGUAGE](data[3][1]).to(device)\n",
    "src_text, trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([37, 1]), torch.Size([43, 1]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = src_text.reshape(-1, 1)  #because batch_size is 1\n",
    "trg_text = trg_text.reshape(-1, 1)\n",
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Seq2SeqPackedAttention_general.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 83\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y143sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_general\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39;49mload(save_path))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y143sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model_general\u001b[39m.\u001b[39meval()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y143sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    271\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Seq2SeqPackedAttention_general.pt'"
     ]
    }
   ],
   "source": [
    "model_general.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model_general.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model_general(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 84\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y144sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y144sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m output \u001b[39m=\u001b[39m output[\u001b[39m1\u001b[39m:]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y144sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m output\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "output = output.squeeze(1)\n",
    "output = output[1:]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 75\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y154sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_max \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"
     ]
    }
   ],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 76\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y155sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m output_max\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_max' is not defined"
     ]
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'output_max' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sapnathapa/Documents/AIT/Spring Sem 2023/NLP/MT + Transformer copy.ipynb Cell 78\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y161sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m output_max:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sapnathapa/Documents/AIT/Spring%20Sem%202023/NLP/MT%20%2B%20Transformer%20copy.ipynb#Y161sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(mapping[token\u001b[39m.\u001b[39mitem()])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_max' is not defined"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('you should know that there is a three-hour version of this',\n",
       " 'आपको मालूम होना चाहिए कि इसका तीन घंटे का वरज़न भी है।')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
